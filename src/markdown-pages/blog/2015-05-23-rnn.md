---
author: berekuk
comments: true
date: 2015-05-23 22:10:29+00:00
layout: post
link: https://blog.berekuk.ru/2015/05/24/rnn/
path: /blog/2015/05/24/rnn/
slug: rnn
title: Рекуррентные нейросети
wordpress_id: 47
categories:
- tech
---

Погонял нейросеть на основе кода из [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). На примере истории чата из 75000 сообщений, 6MB данных. Увлекает.

До ИИ там конечно [далеко](http://karpathy.github.io/2012/10/22/state-of-computer-vision/), но способность вычленять структуру из потока байтов впечатляет. Связный разговор не осиливает, связные предложения наугад, а вот остальное очень даже.

Пример (выборка не наиболее удачная, выбранная как средне-репрезентативная + чтобы не спалить ничего приватного):

    
    Слава:
    Хочется, да :)
    
    Пион:
    Я прочувствуешь, что в итоге могут сильнее продолговать.
    
    Пион:
    Не
    
    Слава:
    штоа?
    
    Пион:
    Ничо так. В падику?
    
    Слава:
    Перестал это же в толпе с тобой подумать
    
    Пион:
    Даа, это твой метод напрягать капса с толкой книги не меняет
    
    Слава:
    Ну вот там же.
    
    Пион:
    О, она будет апдейтиться в такой жизни очень хорошее
    
    Слава:
    А ТЫ? :)


Что тут интересно:

	
  * Структуру входящего потока в виде "Пион:"/"Слава:" оно просекает почти сразу. Алгоритм инкрементальный, и проходит 30 "эпох" (полных перечитываний текста), на ранних эпохах содержание сообщений не содержит читаемых слов (_"я центрить назачиваешь аэтри, а КАВ НЕТЕСТА ПВО НИВ У ОЕД ЧТО У ТЕ СЛЕДАЮ УНОДАСЙ РЕМ"_).

  * Аналогично со структурой предложений: пробел после запятой, длина слова.
	
  * Еще до этого оно осознало структуру уникода: изначальный алгоритм работает с байтами, а на выходе получаются символы. То есть \xd0 и \xd1 оно расставляет аккуратно и в тему.

  * Капс, смайлы, точки, сообщения с маленьких букв и с больших (я за этим не очень слежу, статистически правдоподобно, хотя в реальности я более последователен в этом на локальных интервалах.)

  * Склонение слов! В оригинале не было слова "прочувствовать", только "прочувствованное", "прочувствованные" и "прочувствовала" по одному разу.

  * Из наблюдения про склонение слов я делаю вывод, что структуру предложений, согласование существительных, глаголов и прилагательных оно тоже понимает на достаточно продвинутом уровне. В смысле, не просто по принципу цепей Маркова.

  * В примере этого нет, но я много использую скобки, и он обычно не забывает их закрывать.

  * Про авторский стиль: некоторые признаки того, что "Слава" выражается более похоже на меня, а "Пион" более похоже на Пион есть. Я чаще ставлю смайлы и скобки. При этом с родом есть косяки.


PS: Заодно опробовал Spot Instances на EC2. На ноутбуке со встроенной видеокартой недоступна CUDA, и ждать десять часов вчера вечером не хотелось. На EC2 On-Demand g2.2xlarge стоит $0.65/час, а Spot мне обошелся в $0.06/час. (со Spot Instances всегда есть риск, резкий рост цен приведет к потере виртуалки со всеми данными, но он маловероятен был в этом случае.)
